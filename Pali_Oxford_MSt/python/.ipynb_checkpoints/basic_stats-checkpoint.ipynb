{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Statistics\n",
    "\n",
    "The corpus contains the following 12 files, which live in the `corpus` directory at https://github.com/ilonabudapesti/buddhism-nlp/tree/master/Pali_Oxford_MSt/corpus\n",
    "\n",
    "They are plain text files directly taken from the Digital Pali Reader which uses an electronic version of the CTS Edition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk, os, re, pprint\n",
    "from nltk import word_tokenize\n",
    "\n",
    "fileids = os.listdir('../corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 12 files are combined into one large string. The length of this string is 122,497 characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw = ''\n",
    "for fileid in fileids:\n",
    "    f = open('../corpus/' + fileid)\n",
    "    for line in f:\n",
    "        raw += line\n",
    "\n",
    "len(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be more useful to know the number of words rather than number of characters in the corpus. Therefore we tokenize the raw string, which will split the string up into words taking empty space as the delimiting character. We sample a few element.\n",
    "\n",
    "Then we use NLTK's built in `Text` method to turn our list of tokens into an NLTK `Text` format. This gives us access to several built-in methods, such as `concordance`, which lists out the occurance of a specific token in the `Text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "tokens[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = nltk.Text(tokens)\n",
    "text.concordance('samaṇa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a frequency distribution of the text, which will show us how frequently each token (also called *word-type*) occurs.\n",
    "\n",
    "We list the tokens in order of freqency. Punctuation such as ',' and '.' are most frequent appearing 1557 and 1377 times respectively. This is followed by 'ti', 'kho', 'na' 364, 278 and 203 occurrences each. Words which appear with high frequency but carry no semantic meaning are called *stop-words*. Examples of stop-words in English are 'and', 'the', 'it' and many pronouns.\n",
    "\n",
    "The first word in the frequency distribution with semantic meaning is 'bhante', which as a form of address falls within the frequency range of stop-words.\n",
    "\n",
    "Note that words from the Angulimala and Mahagovinda, both from the DN, will appear higher in the frequency distribution due to their texts being longer and therefore there is more chance for repetition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(text)\n",
    "fd.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(text) # number of tokens (words, including punctuation and references) in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We remove punctuation and numbers, and count unique tokens.\n",
    "\n",
    "vocab = sorted(set([w.lower() for w in text if w.isalpha()]))\n",
    "len(vocab) \n",
    "\n",
    "# There are 4,052 unique word-types or tokens in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of all tokens is below, ordered according to the Latin alphabet, with capital letters preceeding small caps.\n",
    "Note that words with different conjugations and declensions count as different tokens. \n",
    "To find out the unique number of head-words we will need to **lemmatize** or **stem** our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text.concordance('aggi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text.concordance('Taṃ') # The concordance method finds occurences regardless of capitalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Letter Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "mpl.rcParams['font.family'] = 'Arial'\n",
    "\n",
    "raw[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# alpha only tokens\n",
    "alphatokens = [t for t in tokens if t.isalpha()]\n",
    "raw2 = ('').join(alphatokens)\n",
    "raw3 = [s.lower() for s in raw2]\n",
    "fd = nltk.FreqDist(raw3)\n",
    "fd.plot()\n",
    "fd.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note there is only one occurance of 'ś', which is strange. The occurance of 'f' and 'w' are from bracketed references pointing to variant readings probably.\n",
    "\n",
    "### Question: Should we remove content in brackets before processing? () {} []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zipf's Law in Pāli\n",
    "\n",
    "Let `f(w)` be the frequency of a word w in free text. Suppose that all the words of a text are ranked according to their frequency, with the most frequent word first. Zipf's law states that the frequency of a word type is inversely proportional to its rank (i.e. `f × r = k`, for some constant `k`). For example, the 50th most common word type should occur three times as frequently as the 150th most common word type.\n",
    "\n",
    "Zipf's law hold empirically true for English, the larger the sample size the better the fit, but not for randomly generated texts using the English alphabet.\n",
    "\n",
    "The question is whether Zipf's law holds true for Pāli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
